run_tag: '2048'

env_config: {
  env_type: '2048'
}
model_config: { # overwritten if loading a checkpoint
  input_size: [1, 4, 4],
  policy_size: 4,
  res_channels: 8,
  res_blocks: 2,
  value_head_res_channels: 8,
  value_head_res_blocks: 1,
  policy_head_res_channels: 8,
  policy_head_res_blocks: 1,
  kernel_size: 3,
  policy_fc_size: 32,
  value_fc_size: 32,
  value_output_activation: ""
}
train_mode_config: {
  algo_type: "lazyzero",
  algo_config: {
    temperature: 1.0,
    num_policy_rollouts: 10, # number of policy rollouts to run per evaluation call
    rollout_depth: 3, # depth of each policy rollout, once this depth is reached, return the network's evaluation (value head) of the state
    puct_coeff: 1.0 # C-value in PUCT formula
  },
  learning_rate: 0.0001,
  replay_memory_max_size: 10000,
  replay_memory_min_size: 10,
  parallel_envs: 8,
  policy_factor: 1.0,
  minibatch_size: 1024,
  minibatches_per_update: 1,
  episodes_per_epoch: 10,
  test_config: {
    episodes_per_epoch: 10
  }
}
test_mode_config: {
  episodes_per_epoch: 100
}



